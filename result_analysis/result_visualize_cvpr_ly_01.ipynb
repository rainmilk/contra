{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result_visualize_cvpr_ly.ipynb 前三个代码段更方便一些的平替。\n",
    "\n",
    "命令行样例：\n",
    "\n",
    "```bash\n",
    "\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.25 --noise_type symmetric --dataset cifar-10\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.5 --noise_type symmetric --dataset cifar-10\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.75 --noise_type symmetric --dataset cifar-10\n",
    "\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.25 --noise_type symmetric --dataset flower-102\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.5 --noise_type symmetric --dataset flower-102\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.75 --noise_type symmetric --dataset flower-102\n",
    "\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.25 --noise_type asymmetric --dataset cifar-100\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.5 --noise_type asymmetric --dataset cifar-100\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.75 --noise_type asymmetric --dataset cifar-100\n",
    "\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.25 --noise_type asymmetric --dataset pet-37\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.5 --noise_type asymmetric --dataset pet-37\n",
    "python ./result_analysis/gen_result_visualize_to_csv.py --noise_rate 0.75 --noise_type asymmetric --dataset pet-37\n",
    "\n",
    "\n",
    "````\n",
    "\n",
    "跑完模型后，可以跑上面这个代码生成绘图需要的数据，然后绘图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from configs import settings\n",
    "from core_model.custom_model import ClassifierWrapper, load_custom_model\n",
    "from core_model.dataset import get_dataset_loader\n",
    "from core_model.train_test import model_forward\n",
    "\n",
    "set_noise_ratio = \"0.75\"\n",
    "set_noise_type = \"asymmetric\"\n",
    "# 0.5 对称 CRUL,ELR,GJS,wfisher的名字为efficientnet_s;发现fisher_new,FT_prune_bi没用\n",
    "\n",
    "cifar_model_name = \"efficientnet_s\"\n",
    "other_model_name = \"wideresnet50\"\n",
    "\n",
    "# raw疑似pretrain?\n",
    "set_basic_name = \"pretrain,inctrain\"\n",
    "set_LNL_name = (\n",
    "    \"Coteaching,Coteachingplus,Decoupling,DISC,ELR,GJS,JoCoR,NegativeLearning,PENCIL\"\n",
    ")\n",
    "set_MU_name = \"FT,GA,GA_l1,wfisher\"\n",
    "set_OUR_name = \"CRUL\"\n",
    "\n",
    "set_uni_name = f\"{set_basic_name},{set_LNL_name},{set_MU_name},{set_OUR_name}\"\n",
    "\n",
    "\n",
    "set_dataset = \"pet-37\"\n",
    "set_model_suffix = \"restore\"\n",
    "set_batch_size = 64\n",
    "\n",
    "\n",
    "def execute():\n",
    "    case = settings.get_case(set_noise_ratio, set_noise_type)\n",
    "    uni_names = set_uni_name\n",
    "    uni_names = [uni_names] if uni_names is None else uni_names.split(\",\")\n",
    "    num_classes = settings.num_classes_dict[set_dataset]\n",
    "\n",
    "    _, _, test_loader = get_dataset_loader(\n",
    "        set_dataset,\n",
    "        \"test\",\n",
    "        None,\n",
    "        batch_size=set_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    _, _, noisy_loader = get_dataset_loader(\n",
    "        set_dataset,\n",
    "        \"train_noisy\",\n",
    "        case,\n",
    "        batch_size=set_batch_size,\n",
    "        shuffle=False,\n",
    "        label_name=\"train_noisy_true_label\",\n",
    "    )\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    for uni_name in uni_names:\n",
    "        print(f\"Evaluating {uni_name}:\")\n",
    "        dict_temp = {}\n",
    "        model_name = cifar_model_name\n",
    "        if set_dataset == \"cifar-10\" or set_dataset == \"cifar-100\":\n",
    "            model_name = cifar_model_name\n",
    "        else:\n",
    "            model_name = other_model_name\n",
    "\n",
    "        loaded_model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "        model = ClassifierWrapper(loaded_model, num_classes)\n",
    "\n",
    "        if uni_name == \"pretrain\":\n",
    "            model_ckpt_path = settings.get_ckpt_path(\n",
    "                set_dataset,\n",
    "                \"\",\n",
    "                model_name,\n",
    "                model_suffix=\"pretrain\",\n",
    "                unique_name=uni_name,\n",
    "            )\n",
    "        elif uni_name == \"inctrain\":\n",
    "            model_ckpt_path = settings.get_ckpt_path(\n",
    "                set_dataset,\n",
    "                case,\n",
    "                model_name,\n",
    "                model_suffix=\"inc_train\",\n",
    "                unique_name=\"\",\n",
    "            )\n",
    "        else:\n",
    "            model_ckpt_path = settings.get_ckpt_path(\n",
    "                set_dataset,\n",
    "                case,\n",
    "                model_name,\n",
    "                model_suffix=set_model_suffix,\n",
    "                unique_name=uni_name,\n",
    "            )\n",
    "        print(f\"Loading model from {model_ckpt_path}\")\n",
    "        checkpoint = torch.load(model_ckpt_path)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        # print(f\"Evaluating test_data:\")\n",
    "        # results, embedding = model_test(test_loader, model)\n",
    "        # print(\"Results: %.4f\" % results)\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Evaluating train_noisy_data:\")\n",
    "        n_results, n_embedding = model_test(noisy_loader, model)\n",
    "        # print(\"Results: %.4f\" % results)\n",
    "        # print(\"Results: \", n_results)\n",
    "        dict_temp = {\"uni_name\": uni_name, **n_results}\n",
    "        results_data.append(dict_temp)\n",
    "\n",
    "    df = pd.DataFrame(results_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def model_test(data_loader, model, device=\"cuda\"):\n",
    "    eval_results = {}\n",
    "\n",
    "    predicts, probs, embedding, labels = model_forward(\n",
    "        data_loader, model, device, output_embedding=True, output_targets=True\n",
    "    )\n",
    "\n",
    "    # global acc\n",
    "    global_acc = np.mean(predicts == labels)\n",
    "    eval_results[\"global\"] = global_acc.item()\n",
    "\n",
    "    # error_rate\n",
    "    eval_results[\"error_rate\"] = 1 - eval_results[\"global\"]\n",
    "    # class acc\n",
    "    label_list = sorted(list(set(labels)))\n",
    "    for label in label_list:\n",
    "        cls_index = labels == label\n",
    "        class_acc = np.mean(predicts[cls_index] == labels[cls_index])\n",
    "        eval_results[\"label_\" + str(label.item())] = class_acc.item()\n",
    "\n",
    "    return eval_results, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /nvme/szh/code/tta-mr/data/pet-37/gen/test_data.npy\n",
      "Loading /nvme/szh/code/tta-mr/data/pet-37/gen/nr_0.75_nt_asymmetric_cvpr/train_noisy_data.npy\n",
      "Evaluating pretrain:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/pretrain/wideresnet50_pretrain.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0320069b7983>:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_ckpt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating train_noisy_data:\n",
      "Evaluating inctrain:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/wideresnet50_inc_train.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating Coteaching:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/Coteaching/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating Coteachingplus:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/Coteachingplus/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating Decoupling:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/Decoupling/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating DISC:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/DISC/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating ELR:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/ELR/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating GJS:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/GJS/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating JoCoR:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/JoCoR/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating NegativeLearning:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/NegativeLearning/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating PENCIL:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/PENCIL/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating FT:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/FT/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating GA:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/GA/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating GA_l1:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/GA_l1/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating wfisher:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/wfisher/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n",
      "Evaluating CRUL:\n",
      "Loading model from /nvme/szh/code/tta-mr/ckpt/pet-37/nr_0.75_nt_asymmetric_cvpr/CRUL/wideresnet50_restore.pth\n",
      "Evaluating train_noisy_data:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results=pd.DataFrame()\n",
    "results=execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_csv('visualize_results_cvpr/cifar10_0.5_sym.csv')\n",
    "# results.to_csv('visualize_results_cvpr/cifar10_0.25_sym.csv')\n",
    "#results.to_csv('visualize_results_cvpr/cifar10_0.75_sym.csv')\n",
    "\n",
    "# results.to_csv('visualize_results_cvpr/flower_0.5_sym.csv')\n",
    "#results.to_csv('visualize_results_cvpr/flower_0.25_sym.csv')\n",
    "# results.to_csv('visualize_results_cvpr/flower_0.75_sym.csv')\n",
    "\n",
    "\n",
    "#results.to_csv('visualize_results_cvpr/cifar100_0.5_asym.csv')\n",
    "#results.to_csv('visualize_results_cvpr/cifar100_0.25_asym.csv')\n",
    "# results.to_csv('visualize_results_cvpr/cifar100_0.75_asym.csv')\n",
    "\n",
    "# results.to_csv('visualize_results_cvpr/pet_0.5_asym.csv')\n",
    "# results.to_csv('visualize_results_cvpr/pet_0.25_asym.csv')\n",
    "results.to_csv('visualize_results_cvpr/pet_0.75_asym.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set corresponding color for each uni_name\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_basic_names = [\"retrain\", \"retrain_ls\", \"retrain_sam\", \"raw\"]\n",
    "set_LNL_names = [\n",
    "    \"Coteaching\",\n",
    "    \"Coteachingplus\",\n",
    "    \"Decoupling\",\n",
    "    \"DISC\",\n",
    "    \"ELR\",\n",
    "    \"GJS\",\n",
    "    \"JoCoR\",\n",
    "    \"NegativeLearning\",\n",
    "    \"PENCIL\",\n",
    "]\n",
    "set_MU_names = [\"finetune\", \"FT\", \"FT_l1\", \"FT_prune\", \"GA\", \"GA_l1\", \"wfisher\"]\n",
    "set_OUR_names = [\"CRUL\"]\n",
    "\n",
    "# 生成配色方案\n",
    "set_basic_colors = plt.cm.Blues(np.linspace(0.3, 0.5, len(set_basic_names)))\n",
    "set_LNL_colors = plt.cm.Greens(np.linspace(0.3, 0.8, len(set_LNL_names)))\n",
    "set_MU_colors = plt.cm.Purples(np.linspace(0.3, 0.7, len(set_MU_names)))\n",
    "set_OUR_colors = [\"#ff1493\"]  # 深粉色调\n",
    "\n",
    "# 创建字典\n",
    "uni_color_dict = {}\n",
    "# 将颜色和 uni_name 配对\n",
    "for name, color in zip(set_basic_names, set_basic_colors):\n",
    "    uni_color_dict[name] = color\n",
    "for name, color in zip(set_LNL_names, set_LNL_colors):\n",
    "    uni_color_dict[name] = color\n",
    "for name, color in zip(set_MU_names, set_MU_colors):\n",
    "    uni_color_dict[name] = color\n",
    "for name, color in zip(set_OUR_names, set_OUR_colors):\n",
    "    uni_color_dict[name] = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# dataframes = [results,results,results,results] \n",
    "# labels = ['Cifar-10', 'Cifar-100', 'Flower-102', 'Oxford-IIIT Pet'] \n",
    "# fig, axs = plt.subplots(2, 2, figsize=(12, 8)) \n",
    "\n",
    "# for i, (df, label) in enumerate(zip(dataframes, labels)):\n",
    "#     ax = axs[i // 2, i % 2]\n",
    "#     for j, (method, global_val) in enumerate(zip(df['uni_name'], df['global'])):\n",
    "#         ax.bar(method, global_val, color=uni_color_dict[method], label=method if i == 0 else \"\",width=0.9) \n",
    "#     ax.set_title(f'{label}')\n",
    "#     # ax.set_xlabel('Algorithm') \n",
    "#     ax.set_ylabel('Global Value')\n",
    "#     ax.set_xticks([])\n",
    "#     # ax.grid(True,linestyle='--',alpha=0.7)\n",
    "\n",
    "# handles, labels = [], [] \n",
    "# for ax in axs.flat: \n",
    "#     h, l = ax.get_legend_handles_labels() \n",
    "#     handles.extend(h) \n",
    "#     labels.extend(l) \n",
    "\n",
    "# n_cols = 4 # 图例列数 \n",
    "# fig.legend(handles, labels, loc='upper center', ncol=n_cols, bbox_to_anchor=(0.5, -0.1), title=\"Methods\")\n",
    "\n",
    "# #整体标题\n",
    "# fig.suptitle('ERROR RATE ON DIFFERENT DATASETS')\n",
    "\n",
    "# # 调整布局 \n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "# fig.set_dpi(300)\n",
    "# # 留出顶部空间给图例 \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #柱状图\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "# from math import pi\n",
    "# import numpy as np\n",
    "\n",
    "# # Defining method categories\n",
    "# lnl_methods = ['Coteaching', 'Coteachingplus', 'Decoupling', 'NegativeLearning', 'PENCIL']\n",
    "# unlearning_methods = ['raw', 'GA', 'FT', 'FT_l1', 'FT_prune', 'retrain', 'retrain_ls', 'retrain_sam']\n",
    "\n",
    "# baseline_methods = ['pretrain', 'inc_train'] \n",
    "\n",
    "# datasets = ['cifar-10_sym', 'cifar-100_asym', 'flower-102_sym', 'pet-37_asym']\n",
    "# display_name_dataset = {\n",
    "#     'cifar-10_sym' : 'Cifar-10', \n",
    "#     'cifar-100_asym': 'Cifar-100', \n",
    "#     'flower-102_sym': 'Flower-102', \n",
    "#     'pet-37_asym' : 'Oxford-IIIT Pet'\n",
    "# }\n",
    "\n",
    "# display_name_legend = {\n",
    "#     'pretrain' : 'PreTrain', \n",
    "#     'inc_train' : 'IncTrain', \n",
    "#     'finetune': 'FineTune', \n",
    "#     'Coteaching' : 'CoTe.', \n",
    "#     'Coteachingplus': 'CoTe.+', \n",
    "#     'Decoupling': 'Decoup.', \n",
    "#     'NegativeLearning' : 'NegLn.', \n",
    "#     'PENCIL' : 'PENCIL', \n",
    "#     'raw': 'Raw', \n",
    "#     'GA': 'GA', \n",
    "#     'FT': 'FT', \n",
    "#     'FT_l1': 'FT-$l_{1}$', \n",
    "#     'FT_prune' : 'FT$_p$', \n",
    "#     'retrain' : 'ReT', \n",
    "#     'retrain_ls' : 'ReT$_L$', \n",
    "#     'retrain_sam' : 'ReT$_S$', \n",
    "#     'CRUL' : 'CRUL'\n",
    "# }\n",
    "\n",
    "# # Creating a bar chart to compare accuracy across methods for each dataset\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "# axes = axes.flatten()\n",
    "# width = 0.7  # Increased the width of the bars for better visibility\n",
    "# tfs = 12 # x/y-ticks font size\n",
    "# title_size = 18 # Title font size\n",
    "# lfs = 14 # Legend-font-size\n",
    "\n",
    "# # Colors for different categories\n",
    "# baseline_colors = ['lightgrey', 'darkgrey', 'silver']\n",
    "# lnl_colors = plt.cm.Blues(np.linspace(0.4, 1, 5))\n",
    "# unlearning_colors = plt.cm.Greens(np.linspace(0.4, 1, 8))\n",
    "# crul_color = 'red'\n",
    "\n",
    "\n",
    "# # Plotting each dataset in a separate subplot\n",
    "# for idx, dataset in enumerate(datasets):\n",
    "#     ax = axes[idx]\n",
    "#     x = np.arange(len(baseline_methods) + len(lnl_methods) + len(unlearning_methods) + 1)  # the label locations\n",
    "    \n",
    "#     # Plotting baseline methods (pretrain, noise-train, finetune)\n",
    "#     for i, method in enumerate(baseline_methods):\n",
    "#         ax.bar(x[i], sheet1_df.loc[dataset, method], width, label=method, color=baseline_colors[i], edgecolor='black', hatch=['//', '..'][i]) # 仅保留 pretrain和inc_train\n",
    "\n",
    "#     # Plotting LNL methods\n",
    "#     for i, method in enumerate(lnl_methods):\n",
    "#         ax.bar(x[len(baseline_methods) + i], sheet1_df.loc[dataset, method], width, label=method, color=lnl_colors[i], edgecolor='black')\n",
    "\n",
    "#     # Plotting Unlearning methods\n",
    "#     for i, method in enumerate(unlearning_methods):\n",
    "#         ax.bar(x[len(baseline_methods) + len(lnl_methods) + i], sheet1_df.loc[dataset, method], width, label=method, color=unlearning_colors[i], edgecolor='black')\n",
    "\n",
    "#     ax.bar(x[-1], sheet1_df.loc[dataset, 'CRUL'], width, label='CRUL', color=crul_color, edgecolor='black')\n",
    "\n",
    "#     ax.set_title(f'{display_name_dataset[dataset]}', fontsize=title_size, fontfamily=\"serif\", weight=\"bold\")\n",
    "\n",
    "#     ax.set_xticks([]) # 关闭x轴显示\n",
    "\n",
    "\n",
    "# # --------------------------------------------------------------------------------- #\n",
    "# \"\"\"\n",
    "# 手动修正四个子图各自的坐标值范围。\n",
    "# # \"\"\"\n",
    "# # # Cifar-10\n",
    "# axes[0].set_ylim(40, 85)\n",
    "# axes[0].set_yticks(np.arange(40, 91, 15))\n",
    "# axes[0].set_yticklabels(\n",
    "#             np.arange(40, 91, 15),\n",
    "#             fontsize=tfs,\n",
    "#             rotation=90,\n",
    "#             va=\"center\",\n",
    "#             fontfamily=\"serif\",\n",
    "#             weight=\"bold\",\n",
    "#         )\n",
    "\n",
    "\n",
    "# # Cifar-100\n",
    "# axes[1].set_ylim(50, 70)\n",
    "# axes[1].set_yticks(np.arange(50, 71, 5))\n",
    "# axes[1].set_yticklabels(\n",
    "#             np.arange(50, 71, 5),\n",
    "#             fontsize=tfs,\n",
    "#             rotation=90,\n",
    "#             va=\"center\",\n",
    "#             fontfamily=\"serif\",\n",
    "#             weight=\"bold\",\n",
    "#         )\n",
    "\n",
    "# # Flower-102\n",
    "# axes[2].set_ylim(50, 95)\n",
    "# axes[2].set_yticks(np.arange(50, 96, 15))\n",
    "# axes[2].set_yticklabels(\n",
    "#             np.arange(50, 96, 15),\n",
    "#             fontsize=tfs,\n",
    "#             rotation=90,\n",
    "#             va=\"center\",\n",
    "#             fontfamily=\"serif\",\n",
    "#             weight=\"bold\",\n",
    "#         )\n",
    "\n",
    "\n",
    "# # Pet-37\n",
    "# axes[3].set_ylim(70, 95)\n",
    "# axes[3].set_yticks(np.arange(70, 96, 10))\n",
    "# axes[3].set_yticklabels(\n",
    "#             np.arange(70, 96, 10),\n",
    "#             fontsize=tfs,\n",
    "#             rotation=90,\n",
    "#             va=\"center\",\n",
    "#             fontfamily=\"serif\",\n",
    "#             weight=\"bold\",\n",
    "#         )\n",
    "\n",
    "\n",
    "# handles, legend_labels = fig.axes[0].get_legend_handles_labels()\n",
    "\n",
    "# labels = [display_name_legend[ll] for ll in legend_labels]\n",
    "\n",
    "# legend_font_properties = fm.FontProperties(\n",
    "#                                         #    weight='bold', \n",
    "#                                            size=lfs)\n",
    "\n",
    "# from matplotlib.lines import Line2D\n",
    "# empty_handle = Line2D([], [], color='none')  # 创建一个空的占位符\n",
    "\n",
    "# # 在第二个 legend 之后插入占位符\n",
    "# handles.insert(2, empty_handle)\n",
    "# labels.insert(2, '')  # 对应的标签留空\n",
    "\n",
    "# fig.legend(handles, labels, loc='lower center', \n",
    "#            bbox_to_anchor=(0.5, -0.05), ncol=6,\n",
    "#            prop=legend_font_properties)\n",
    "\n",
    "# fig.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #雷达图\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from math import pi\n",
    "\n",
    "# # CIFAR-10的标签\n",
    "# labels = ['label0', 'label1', 'label2', 'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9']\n",
    "\n",
    "# # 假设数据\n",
    "# our_method_scores = [0.85, 0.88, 0.82, 0.80, 0.83, 0.87, 0.84, 0.86, 0.81, 0.89]\n",
    "# resnet18_scores = [0.75, 0.77, 0.74, 0.70, 0.73, 0.78, 0.76, 0.75, 0.72, 0.74]\n",
    "\n",
    "# # 设置雷达图的角度\n",
    "# num_vars = len(labels)\n",
    "\n",
    "# # 角度计算（每个标签对应一个角度）\n",
    "# angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# # 将数据循环到开始位置，这样雷达图会闭合\n",
    "# our_method_scores += our_method_scores[:1]\n",
    "# resnet18_scores += resnet18_scores[:1]\n",
    "# angles += angles[:1]\n",
    "\n",
    "# # 创建雷达图\n",
    "# fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# # 画 Our method 和 ResNet18 的雷达图\n",
    "# ax.plot(angles, our_method_scores, color='blue', linewidth=2, label='Our method')\n",
    "# ax.fill(angles, our_method_scores, color='blue', alpha=0.25)\n",
    "\n",
    "# ax.plot(angles, resnet18_scores, color='red', linewidth=2, label='ResNet18')\n",
    "# ax.fill(angles, resnet18_scores, color='red', alpha=0.25)\n",
    "\n",
    "# # 设置标签（CIFAR-10的类别）\n",
    "# ax.set_yticklabels([])\n",
    "# ax.set_xticks(angles[:-1])\n",
    "# ax.set_xticklabels(labels, fontsize=10)\n",
    "\n",
    "# # 添加标题\n",
    "# ax.set_title('Comparison of Our method and ResNet18 on CIFAR-10', size=14, color='black', y=1.1)\n",
    "\n",
    "# # 添加图例\n",
    "# ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.05))\n",
    "\n",
    "# # 显示图形\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 方法名和对应的准确率（根据实际结果替换这些数据）\n",
    "# methods = ['Our method', 'ResNet18', 'Method A', 'Method B']\n",
    "# accuracies = [0.85, 0.75, 0.78, 0.80] # 假设这四种方法在 CIFAR-10 上的准确率\n",
    "\n",
    "# # 设置柱状图的宽度和位置\n",
    "# x = np.arange(len(methods)) # 方法的数量\n",
    "# width = 0.5 # 每个柱子的宽度\n",
    "\n",
    "# # 创建柱状图\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# # 绘制柱状图\n",
    "# bars = ax.bar(x, accuracies, width, color=['blue', 'red', 'green', 'orange'])\n",
    "\n",
    "# # 设置标签和标题\n",
    "# ax.set_xlabel('Methods', fontsize=12)\n",
    "# ax.set_ylabel('Accuracy', fontsize=12)\n",
    "# ax.set_title('Comparison of Different Methods on CIFAR-10', fontsize=14)\n",
    "# ax.set_xticks(x) # 设置 X 轴刻度位置\n",
    "# ax.set_xticklabels(methods, fontsize=12) # 设置 X 轴标签\n",
    "\n",
    "# # 添加柱状图的数值标签\n",
    "# for bar in bars:\n",
    "#     height = bar.get_height()\n",
    "#     ax.annotate(f'{height:.2f}', # 显示每个柱的准确率\n",
    "#     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "#     xytext=(0, 3), # 偏移量\n",
    "#     textcoords=\"offset points\",\n",
    "#     ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# # 显示图形\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from configs import settings\n",
    "import logging\n",
    "from core_model.custom_model import ClassifierWrapper, load_custom_model\n",
    "from core_model.dataset import get_dataset_loader, BaseTensorDataset\n",
    "from core_model.train_test import model_forward\n",
    "\n",
    "# 设置设备和日志\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_features(feature_extractor, data_loader):\n",
    "    \"\"\"\n",
    "    提取数据的嵌入特征和标签。\n",
    "    \"\"\"\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Extracting Features\"):\n",
    "            images = images.to(device)\n",
    "            outputs = feature_extractor(images).view(images.size(0), -1)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    try:\n",
    "        return np.concatenate(features), np.concatenate(labels)\n",
    "    except ValueError:\n",
    "        logger.error(\"Error concatenating features or labels; check data loader and model output.\")\n",
    "        return None, None\n",
    "\n",
    "def retrieve(gallery_feats, query_feats, top_k=10):\n",
    "    \"\"\"\n",
    "    计算相似度并返回前 top-k 的索引。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sims = cosine_similarity(query_feats, gallery_feats)\n",
    "        indices = np.argsort(-sims, axis=1)[:, :top_k]\n",
    "        sim_scores = np.take_along_axis(sims, indices, axis=1)\n",
    "        return indices, sim_scores\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Error in similarity calculation: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_map(indices, gallery_labels, query_labels):\n",
    "    \"\"\"\n",
    "    计算 mAP 值。\n",
    "    \"\"\"\n",
    "    if indices is None:\n",
    "        logger.warning(\"No indices to calculate mAP. Skipping.\")\n",
    "        return 0.0\n",
    "    ap_list = []\n",
    "    for i in range(len(query_labels)):\n",
    "        query_label = query_labels[i]\n",
    "        relevant = (gallery_labels[indices[i]] == query_label).astype(int)\n",
    "        num_relevant = relevant.sum()\n",
    "        if num_relevant == 0:\n",
    "            ap_list.append(0)\n",
    "            continue\n",
    "        cumulative_precision = np.cumsum(relevant) / (np.arange(len(relevant)) + 1)\n",
    "        ap = (cumulative_precision * relevant).sum() / num_relevant\n",
    "        ap_list.append(ap)\n",
    "    return np.mean(ap_list) if ap_list else 0.0\n",
    "\n",
    "def visualize_retrieval(query_data, query_labels, gallery_data, gallery_labels, query_idx, retrieved_indices, sim_scores, dataset_name=\"cifar-10\"):\n",
    "    \"\"\"\n",
    "    可视化检索结果，将查询图像与检索结果并列显示。\n",
    "    \"\"\"\n",
    "    if retrieved_indices is None or sim_scores is None:\n",
    "        logger.warning(\"No retrieval results to display. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    fig, axs = plt.subplots(1, len(retrieved_indices) + 1, figsize=(15, 3))\n",
    "    \n",
    "    # 处理查询图像\n",
    "    query_img = recover_img(query_data[query_idx], dataset_name)\n",
    "    axs[0].imshow(np.transpose(query_img, (1, 2, 0)))\n",
    "    axs[0].set_title(f\"Query\\nLabel: {query_labels[query_idx]}\", color=\"purple\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # 处理检索结果\n",
    "    for i, idx in enumerate(retrieved_indices):\n",
    "        img = recover_img(gallery_data[idx], dataset_name)\n",
    "        label = gallery_labels[idx]\n",
    "        axs[i + 1].imshow(np.transpose(img, (1, 2, 0)))\n",
    "        axs[i + 1].set_title(f\"Label: {label} ({sim_scores[i]:.2f})\", color=\"green\" if label == query_labels[query_idx] else \"red\")\n",
    "        axs[i + 1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def recover_img(img, dataset_name):\n",
    "    \"\"\"\n",
    "    恢复归一化的图像，用于可视化。\n",
    "    \"\"\"\n",
    "    if dataset_name == \"cifar-10\":\n",
    "        means, stds = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis], np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]\n",
    "    elif dataset_name == \"cifar-100\":\n",
    "        means, stds = np.array([0.5071, 0.4865, 0.4409])[:, np.newaxis, np.newaxis], np.array([0.2673, 0.2564, 0.2762])[:, np.newaxis, np.newaxis]\n",
    "    elif dataset_name == \"pet-37\":\n",
    "        means, stds = np.array([0.485, 0.456, 0.406])[:, np.newaxis, np.newaxis], np.array([0.229, 0.224, 0.225])[:, np.newaxis, np.newaxis]\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Normalization for dataset {dataset_name} not implemented.\")\n",
    "    return np.clip(img * stds + means, 0, 1)\n",
    "\n",
    "def evaluate_image_retrieval(dataset_name, model_name, noise_ratio, noise_type=\"symmetric\", methods=None, top_k=5, query_indices=None):\n",
    "    \"\"\"\n",
    "    执行图像检索任务，包括 mAP 计算和检索结果的可视化。\n",
    "    \"\"\"\n",
    "    assert methods is not None, \"请指定方法列表\"\n",
    "    assert query_indices is not None, \"请指定查询索引列表\"\n",
    "\n",
    "    case = settings.get_case(noise_ratio, noise_type)\n",
    "    num_classes = settings.num_classes_dict[dataset_name]\n",
    "\n",
    "    # 加载数据集\n",
    "    train_data, train_labels, train_loader = get_dataset_loader(dataset_name, \"train\", None, None, batch_size=64, shuffle=False)\n",
    "    test_data, test_labels, test_loader = get_dataset_loader(dataset_name, \"test\", None, None, batch_size=64, shuffle=False)\n",
    "\n",
    "    for method in methods:\n",
    "        model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "        model = ClassifierWrapper(model, num_classes)\n",
    "\n",
    "        # 加载模型权重\n",
    "        model_path = f\"/nvme/szh/code/tta-mr/ckpt/{dataset_name}/nr_{noise_ratio}_nt_{noise_type}_cvpr/{method}/{model_name}_restore.pth\"\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "            logger.info(f\"Evaluating model at {model_path}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"Model checkpoint {model_path} not found. Skipping method {method}.\")\n",
    "            continue\n",
    "\n",
    "        # 提取特征\n",
    "        feature_extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "        g_features, g_labels = extract_features(feature_extractor, train_loader)\n",
    "        q_features, q_labels = extract_features(feature_extractor, test_loader)\n",
    "\n",
    "        if g_features is None or q_features is None:\n",
    "            logger.warning(\"Feature extraction failed; skipping this method.\")\n",
    "            continue\n",
    "\n",
    "        # 计算 mAP\n",
    "        indices, sim_scores = retrieve(g_features, q_features, top_k)\n",
    "        mAP = calculate_map(indices, g_labels, q_labels)\n",
    "        logger.info(f\"mAP for method {method}: {mAP:.4f}\")\n",
    "\n",
    "        # 可视化检索结果\n",
    "        for query_idx in query_indices:\n",
    "            retrieved_indices, sim_scores = retrieve(g_features, q_features[query_idx:query_idx+1], top_k)\n",
    "            if retrieved_indices is not None and sim_scores is not None:\n",
    "                visualize_retrieval(test_data, test_labels, train_data, train_labels, query_idx, retrieved_indices[0], sim_scores[0], dataset_name)\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_name = \"cifar-10\"\n",
    "    model_name = \"efficientnet_s\"\n",
    "    noise_ratio = 0.1\n",
    "    noise_type = \"symmetric\"\n",
    "    methods = [\"Coteaching\", \"Coteachingplus\", \"JoCoR\"]\n",
    "    query_indices = [0, 1, 2]  # 测试图片索引\n",
    "    evaluate_image_retrieval(dataset_name, model_name, noise_ratio, noise_type, methods=methods, top_k=5, query_indices=query_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
